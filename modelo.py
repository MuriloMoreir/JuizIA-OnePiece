import os
from dotenv import load_dotenv
import google.generativeai as genai
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.document_loaders import TextLoader
from langchain_core.messages import SystemMessage, HumanMessage
from datetime import datetime

load_dotenv()
API_KEY = os.getenv("GEMINI_API_KEY")
if not API_KEY:
    raise ValueError("GEMINI_API_KEY nÃ£o estÃ¡ configurada!")

genai.configure(api_key=API_KEY)

# InicializaÃ§Ã£o dos modelos
llm_tutor = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash",
    temperature=0.7,
    google_api_key=API_KEY
)

llm_juiz = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash",
    temperature=0.3,
    google_api_key=API_KEY
)

embeddings = GoogleGenerativeAIEmbeddings(
    google_api_key=API_KEY,
    model="models/embedding-001"
)

# Prompts
PROMPT_TUTOR = """
VocÃª Ã© um especialista em One Piece. Ao responder:

1. Use a base de conhecimento (.txt) sempre que possÃ­vel e cite a fonte (arquivo e linha).
2. Seja claro e didÃ¡tico, ajustando o nÃ­vel de detalhe:
   - Novatos: definiÃ§Ãµes simples e contexto bÃ¡sico.
   - Veteranos: teorias, conexÃµes e â€œeaster eggsâ€.
3. Ilustre com exemplos precisos (CapÃ­tulo X, Volume Y ou EpisÃ³dio Z).
4. Sugira ao final 1â€“2 recursos para aprofundamento (artigos, vÃ­deos, fan wikis).
5. Se nÃ£o encontrar a informaÃ§Ã£o, responda:  
   â€œNÃ£o hÃ¡ registro nos nossos arquivos. Recomendo consultar [recurso externo].â€
""".strip()

PROMPT_JUIZ = """
VocÃª Ã© o â€œOrÃ¡culo do Thousand Sunnyâ€, avaliador da fidelidade das respostas ao cÃ¢none de One Piece. Para cada resposta, faÃ§a:

1. AderÃªncia ao cÃ¢none (0â€“10)  
2. Clareza e organizaÃ§Ã£o (0â€“10)  
3. Uso correto de referÃªncias (capÃ­tulo/episÃ³dio) (0â€“10)  
4. Profundidade do contexto histÃ³rico e cultural (0â€“10)  
5. Utilidade para novatos e veteranos (0â€“10)  

ğŸ´â€â˜ ï¸ Carimbo do CapitÃ£o:  
âœ… Aprovado para Zarpar / âš ï¸ Revisar na Ilha dos Erros  

ğŸ’° Tesouro de Pontos:  
X/10 Berries  

Detalhamento:  
- AderÃªncia: X  
- Clareza: X  
- ReferÃªncias: X  
- Profundidade: X  
- Utilidade: X  

ComentÃ¡rio do OrÃ¡culo: [resumo conciso dos pontos fortes e fracos]  
Mapas para Aprofundar: [sugestÃµes prÃ¡ticas de melhoria]  
""".strip()


# FunÃ§Ã£o para carregar documentos
def carregar_documentos(pasta_txt: str):
    """
    Carrega todos os .txt da pasta indicada, gera embeddings
    e retorna um objeto RetrievalQA pronto para uso.
    """
    # Descobre caminho absoluto da pasta
    base_dir = os.path.dirname(__file__)
    pasta = os.path.join(base_dir, pasta_txt)
    if not os.path.isdir(pasta):
        raise FileNotFoundError(f"Pasta de conhecimento nÃ£o encontrada: {pasta}")

    # Carrega e concatena documentos
    documentos = []
    for arquivo in os.listdir(pasta):
        if arquivo.lower().endswith(".txt"):
            caminho = os.path.join(pasta, arquivo)
            loader = TextLoader(caminho, encoding="utf-8")
            documentos.extend(loader.load())

    # Divide em chunks
    splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    docs_divididos = splitter.split_documents(documentos)

    # Cria Ã­ndice FAISS
    db = FAISS.from_documents(docs_divididos, embeddings)

    # Cadeia RAG: usa o tutor como LLM
    rag_chain = RetrievalQA.from_chain_type(
        llm=llm_tutor,
        retriever=db.as_retriever(),
        return_source_documents=True
    )
    return rag_chain

# Carrega o RAG ao importar o mÃ³dulo
# Ajuste o nome da pasta conforme sua estrutura de pastas
RAG_CHAIN = carregar_documentos("Base de Conhecimento")

# --- FunÃ§Ã£o principal de resposta -----------------
def responder_pergunta(pergunta: str) -> str:
    """
    Processa uma pergunta:
    1) RAG busca resposta nos docs
    2) Juiz avalia a resposta
    3) Retorna texto formatado
    """
    # 1) Obter resposta via RAG
    rag_output = RAG_CHAIN.invoke({"query": pergunta})
    resposta_texto = rag_output["result"].strip()
    fontes = [doc.metadata.get("source", "") for doc in rag_output["source_documents"]]

    # 2) AvaliaÃ§Ã£o pelo juiz
    mensagens_juiz = [
        SystemMessage(content=PROMPT_JUIZ),
        HumanMessage(content=f"Pergunta: {pergunta}\nResposta: {resposta_texto}")
    ]
    avaliacao = llm_juiz.invoke(mensagens_juiz).content.strip()

    # 3) FormataÃ§Ã£o final
    resposta_final = (
        "ğŸï¸ RelatÃ³rio da ExpediÃ§Ã£o ao Mundo de One Piece ğŸï¸\n\n"
        "ğŸ—¨ï¸ Resposta do Especialista:\n"
        f"{resposta_texto}\n\n"
        "âš–ï¸ Carimbo do CapitÃ£o (AvaliaÃ§Ã£o do OrÃ¡culo):\n"
        f"{avaliacao}\n\n"
        "ğŸš¢ PrÃ³ximo Destino:\n"
        "- FaÃ§a outra pergunta para continuar sua jornada pelos mares de One Piece.\n\n"
        "ğŸŒŸ Que os ventos da Grand Line guiem suas descobertas! ğŸŒŸ"
    )
    return resposta_final
